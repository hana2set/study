# 선형 회귀
- 특성(입력)과 라벨(출력) 간의 선형 관계를 모델링하여 예측하는 통계 기법
- 데이터를 통해 최적선을 그림
  ![image](https://github.com/user-attachments/assets/22712076-18ec-4b81-abdf-f0a9bc5921ef)

## 선형 회귀식
- 단일 특성: `y' = b + w₁x₁`
- 다중 특성: `y' = b + w₁x₁ + w₂x₂ + ... + wₙxₙ`
  - `y'`: 예측값
  - `b`: 편향(bias)
  - `w₁, w₂, ..., wₙ`: 각 특성의 가중치(weights)
  - `x₁, x₂, ..., xₙ`: 입력 특성들


## 손실
- 모델의 예측이 얼마나 잘못되었는지를 나타내는 숫자 항목
   ![image](https://github.com/user-attachments/assets/531794a2-1279-4d37-915f-482d9360f9e1)

### 손실 유형

  | 손실 함수 이름 | 정의 | 수식 표현 |
  |----------------|------|-----------|
  | **L1 손실** (절댓값 손실) | 예측값과 실제값의 차이의 절댓값의 합 | Σ &#124;y - y'&#124; |
  | **MAE** (평균 절대 오차) | L1 손실의 평균 | (1/N) × Σ &#124;y - y'&#124; |
  | **L2 손실** (제곱 손실) | 예측값과 실제값의 차이의 제곱의 합 | Σ (y - y')² |
  | **MSE** (평균 제곱 오차) | L2 손실의 평균 | (1/N) × Σ (y - y')² |
  
  - `MAE`모델: 이상치에 둔감. 대부분의 포인트에 가깝다. -> 간단함, 직관적
    ![image](https://github.com/user-attachments/assets/6510cafe-c25c-45de-b4b3-0b276e19fe05)
  - `MSE`모델: 이상치에 민감. 오차가 커짐 
    ![image](https://github.com/user-attachments/assets/c9440698-3429-4969-ac88-67a67d42842f)
    

## 경사하강법
- 손실이 가장 작은 모델을 생성하는 가중치와 편향을 반복적으로 찾는 수학적 기법
    1. 현재 가중치와 편향으로 손실을 계산
    2. 손실을 줄이는 가중치와 편향을 이동할 방향을 결정
    3. 가중치와 편향 값을 손실을 줄이는 방향으로 조금씩 이동
    4. 1단계로 돌아가 모델이 더 이상 손실을 줄일 수 없을 때까지 이 과정을 반복
- 선형 모델의 손실 함수는 **항상 볼록 곡면을 생성**하기 때문에, 모델이 수렴하면 결국 가장 낮은 손실을 생성하는 값을 찾을 수 있음
  ![image](https://github.com/user-attachments/assets/7056f3ec-66c3-4c02-a2ff-62c4191920c4)
 
## 초매개변수(하이퍼파라미터)
- 사용자가 학습의 다양한 측면을 제어하는 변수
- **매개변수는 사용자가 제어하는 값이 아님. 모델이 계산하는 값**


### 학습률 
- 모델이 수렴하는 속도에 영향을 미치는 부동 소수점 수
  - 작으면 학습 속도 느림, 크면 수렴하지 않고 진동
  - **데이터 세트마다 자체적인 이상적 학습률**이 있음


### 배치 크기
- 갱신할 때 처리할 데이터 세트의 수 (1~N)
  - 대규모 데이터 세트를 일괄 처리하는 것(`전체 배치`)은 비효율적
  - 하나만 하는 경우 (`확률적 경사하강법, SGD`) 노이즈가 많다. (업데이트 과정에 손실이 커짐)
  - 그 절충안으로 임의의 배치 크기(1~N)를 입력 `미니 배치 확률적 경사하강법`

### 에포크
- 모델이 학습 세트의 **모든 예시를 한 번 처리했음**을 의미
- 배치 크기에 에포크 수치가 영향 받음


# 로지스틱 회귀
- 주어진 결과의 확률을 예측하도록 고안된 회귀 모델 (날씨, 스팸 여부 등)
- 매우 효율적인 확률 계산 메커니즘

## 확률 계산
- 선형 방정식의 출력을 시그모이드 함수에 대입 계산
- 바이너리 카테고리로 변환하기 좋음.

### 시그모이드 함수
- S자 형태이며, 항상 0과 1사이의 값을 출력할 수 있음
- 표준 로지스틱 함수 라고도 하며 수식은 다음과 같음.
  
$$
f(x) = \frac{1}{1 + e^{-x}}
$$

![image](https://github.com/user-attachments/assets/cf2eaa37-688c-4f84-b28f-0f004bef4811)

#### 선형 출력을 변환

$$
z = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b
$$

- $z$: 선형 방정식의 출력으로, $log-odds$라고도 함
- $w_i$: 각 특성에 대한 가중치  
- $x_i$: 특성 값  
- $b$: 편향

이 결과 $z$를 시그모이드 함수에 입력하여 계산
![image](https://github.com/user-attachments/assets/c961f327-506e-4684-8e0c-ce4d4f341626)


## 손실
- 선형 회귀 모델과의 차이점
  - 로그 손실 함수 사용 (선형은 제곱 손실)
  - 정규화 적용이 과적합을 방지

#### 로그 손실
$$
\text{Log Loss} = \sum_{(x, y) \in D} -y \log(y') - (1 - y) \log(1 - y')
$$

- $y$: 로지스틱 회귀임으로, 0 또는 1
- $y'$: 모델의 예측값 (0 ~ 1)


## 정규화
- 모델 복잡성에 패널티를 적용하는 메커니즘. (복잡성을 줄이도록 함)
- 없으면 손실이 0으로 계속 이동
- 따라서 두가지 방식을 채용
  - L_2 정규화
  - 조기 중단





----

출처:  
[Crash Course > 선형 회귀](https://developers.google.com/machine-learning/crash-course/linear-regression?hl=ko)  
[Python 예제 - Keras 라이브러리](https://developers.google.com/machine-learning/crash-course/linear-regression/programming-exercise?hl=ko)
