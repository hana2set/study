# Large language models (LLMs)


## 언어 모델(Language Model)이란?
> 토큰: 단어를 의미가 있는 하위 단어로 분류한 각 덩어리
>
> 예시: unwatched  
> - un (the prefix)
> - watch (the root)
> - ed (the suffix)

  - 상대적으로 긴 토큰 시퀀스에서 토큰 또는 토큰 시퀀스가 ​​발생할 확률을 추정하는 모델
  - 직관에 어긋나지만 텍스트를 평가하는 모델(텍스트 분류, 감정 분석 등)은 언어 모델이 아님
- 시퀸스에서 빈칸에 무엇이 들어갈지 추정하는 작업은 생각보다 복잡한 작업
  - 텍스트 생성
  - 다른 언어로 번역
  - 문서 요약

### N-gram 언어 모델
- N: 시퀸스에 포함된 단어 개수
- N이 클수록 **문맥(Context)** 파악에 용이.
  - 하지만 N이 너무 크면 인스턴스의 발생 빈도가 감소 -> 대상 토큰을 예측하는데 도움이 되지 않음

### 순환 신경망 (Recurrent neural networks)
- N-gram 보다 더 많은 문맥 제공
- vanishing gradient problem 에 유의

### 대규모 언어 모델(LLMs)
- 매우 많은 매개변수를 갖는 언어모델
  - 순환 신경망보다 더 많은 매개변수, 문맥을 수집
- 예: Transformer


## 트랜스포머 (대표적인 LLMs 예시)
- LLM의 일종으로 시퀀스-투-시퀀스(sequence-to-sequence) 과제를 수행하기 위한 모델.
- 대표적으로 기계 번역.
- 인코더와 디코더로 구성됨

### 인코더와 디코더
- 인코더, 디코더 둘다 거대 신경망
- 인코더는 입력 텍스트를 중간 표현으로, 디코더는 중간 표현을 유용한 텍스트로 변환



### 셀프 어텐션(self-attention or self-attention layer)
> 어텐션이란?
> 기계학습의 일종으로 시퀸스 요소들 가운데 중요한 요소에만 집중하고 그렇지 않은 요소는 무시하는 기법
- 시퀸스에서 "자기 자신(토큰)"과 나머지 토큰 간의 관계의 중요성에만 가중치를 부여함
- 임베딩 시퀸스를 다른 임베딩 시퀸스로 변환하는 신경망 계층
- transformer 경쟁력의 원천!

#### 멀티헤드 셀프 어텐션(multi-head self-attention)
- 여러 번의 독자적인(multi-head) 셀프 어텐션 수행을 가르킴

## LLM의 장단점
### 이점
- 다양한 독자층
- 명확하고 이해하기 쉬운 텍스트 생성
- 명시적으로 훈련받은 과제에 대한 예측

### 문제점
- 학습
  - 엄청난 양의 훈련 세트를 수집
  - 엄청난 양의 컴퓨팅 리소스와 전력을 소모
  - 병렬성 문제 해결
- 예측
  - 환각(hallucinate)
  - 엄청난 양의 컴퓨팅 리소스와 전력을 소모


 ## 미세 조정(Fine-tuning)
 - 사전 훈련된 모델에서 특정 과제에 대한 매개변수를 개선하기 위해 수행되는 두 번째 훈련
 - LLM을 실용적으로 만듬
 - 비교적 적은 학습 데이터 사용하지만 비용은 높음


## 증류
- 모델의 크기를 줄이는 작업 (LLM의 축소 버전 생성)
- 더 빠른 예측
- 메모리 및 에너지 사용량 감소
- 성능 감소

## 프롬프트 엔지니어링 (= prompt design)
- LLM에서 원하는 응답을 이끌어내는 프롬프트를 만드는 기술
- 예시
  - 원샷 프롬프팅(one-shot prompting): 질문에 어떻게 응답해야 하는지 보여주는 예시를 포함
  - 퓨샷 프롬프팅(few-shot prompting): 여러 개의 예시를 포함
  - 제로샷 프롬프팅(zero-shot prompting): 예시를 포함하지 않음
 
## 오프라인 추론 (Offline inference, bulk inference, static inference)
- LLM 매개변수가 너무 많아, 추론이 너무 오래 걸리는 경우, 실제 작업에 적용하기 어려운 경우가 있음.
- **모델이 예측을 배치로 생성 하고 해당 예측을 캐싱(저장)하는 프로세스**
- 예: Google 검색이 50개 이상의 언어로 된 800개 이상의 코로나 백신 동의어 목록을 캐싱함.


----

https://developers.google.com/machine-learning/crash-course/llm
